---
title: ""
date: 2019-07-28T23:32:34-04:00
---

<img class="profile-picture" src="irene.jpg">

I'm a Ph.D. candidate at MIT CSAIL, advised by [Professor David Sontag](https://people.csail.mit.edu/dsontag/) in the [Clinical Machine Learning](http://clinicalml.org) group. My research focuses on robust machine learning for real-world problems, particularly on topics of healthcare and inequality.

Before my PhD, I completed a joint AB/SM degree at Harvard. I also worked at Dropbox as a data scientist, machine learning engineer, and chief of staff.

You can reach me at iychen [at] mit [dot] edu or on [Twitter](http://www.twitter.com/irenetrampoline).

<br>

## Publications

<script>
function absCHF() {
    var x = document.getElementById("abs-fairness");
    if (x.style.display === "none") {
        x.style.display = "block";
    } else {
        x.style.display = "none";
    }
}
</script>

#### Conferences
**Robustly Extracting Medical Knowledge from EHRs: A Case Study of Learning a Health Knowledge Graph.** 
<br>
Irene Y. Chen, Monica Agrawal, Steven Horng, David Sontag. 
<br>
*PSB 2020*, <b><font color="#B03A2E">Oral Presentation</font></b>.

**Why Is My Classifier Discriminatory?** 
<br>
Irene Y. Chen, Fredrik D. Johansson, David Sontag. 
<br>
*NeurIPS 2018*, <b><font color="#B03A2E">Spotlight Presentation (top 4% of submitted papers)</font></b>.
<br>
[<a id="abs-fairness-button" onclick="absCHF()">abstract</a>, [pdf](https://arxiv.org/abs/1805.12002), [slides](/assets/neurips18_slides.pdf), [poster](/assets/neurips18_poster.pdf)]

<div id="abs-fairness" style="display:none;">
<blockquote>Recent attempts to achieve fairness in predictive models focus on the balance between fairness and accuracy. In sensitive applications such as healthcare or criminal justice, this trade-off is often undesirable as any increase in prediction error could have devastating consequences. In this work, we argue that the fairness of predictions should be evaluated in context of the data, and that unfairness induced by inadequate samples sizes or unmeasured predictive variables should be addressed through data collection, rather than by constraining the model. We decompose cost-based metrics of discrimination into bias, variance, and noise, and propose actions aimed at estimating and reducing each term. Finally, we perform case-studies on prediction of income, mortality, and review ratings, confirming the value of this analysis. We find that data collection is often a means to reduce discrimination without sacrificing accuracy.</blockquote>
</div>

**Sources of Unfairness in Intensive Care Unit Mortality Scores.** <br>Irene Y. Chen, Fredrik D. Johansson, David Sontag. <br> *Women in Machine Learning Workshop at NeurIPS 2017.*

#### Journals
**Turning the crank for machine learning: ease, at what expense?**
<br>
Tom J. Pollard, Irene Y. Chen, Jenna Wiens, Steven Horng, Danny Wong, Marzyeh Ghassemi, Heather Mattie, Emily Lindmeer, Trishan Panch.
<br>
*Lancet Digital Health*, September 2019.
<br>
[<a href="https://www.thelancet.com/journals/landig/article/PIIS2589-7500(19)30112-8/fulltext">pdf</a>]

**Practical guidance on artificial intelligence for health-care data.** 
<br>
Marzyeh Ghassemi, Tristan Naumann, Peter Schulam, Andrew L. Beam, Irene Y. Chen, Rajesh Ranganath. 
<br>
*Lancet Digital Health*, August 2019.
<br>
[<a href="https://www.thelancet.com/journals/landig/article/PIIS2589-7500(19)30084-6/fulltext">pdf</a>]

**The Disparate Impacts of Medical and Mental Health with AI.** 
<br>
Irene Y. Chen, Peter Szolovits, Marzyeh Ghassemi. 
<br>
*AMA Journal of Ethics*, February 2019.
<br>
[[pdf](https://journalofethics.ama-assn.org/article/can-ai-help-reduce-disparities-general-medical-and-mental-health-care/2019-02)]

#### Other

**We should treat algorithms like prescription drugs.**
<br>
Andy Coravos, Irene Chen, Ankit Gordhandas, Ariel Dora Stern.
<br>
*Quartz*, February 14, 2019.
<br>
[[link](https://qz.com/1540594/treating-algorithms-like-prescription-drugs-could-reduce-ai-bias/)]


## Teaching

At MIT, I served as a Teaching Assistant in Spring 2019 for [Machine Learning for Healthcare](http://mlhc19mit.github.io). Our class was covered by [MIT News](http://news.mit.edu/2019/want-know-what-software-driven-health-care-looks-mit-class-offers-some-clues-0724).

At Harvard, I was awarded the [Derek Bok Center Certificate of Distinction in Teaching](https://bokcenter.harvard.edu/awards) for outstanding teaching evaluations. I served on the teaching staff of the following Harvard classes:

 * Algorithms and Data Structures, Jelani Nelson
 * Microeconomic Theory, Ed Glaeser
 * Multivariable Calculus, Evelyn Hu and Avi Shapiro
 * Differential Equations, Margo Levine and Avi Shapiro
 * Linear Algebra and Real Analysis I, Paul Bamberg
 * Linear Algebra and Real Analysis II, Paul Bamberg


## Selected Press

MIT News, ["Want to know what software-driven health care looks like? This class offers some clues"](http://news.mit.edu/2019/want-know-what-software-driven-health-care-looks-mit-class-offers-some-clues-0724), Kim Martineau, July 24, 2019.

MIT News, ["The Heart of the Matter"](https://www.youtube.com/watch?v=UZcdAZErPUY), Lillie Paquette, May 10, 2019.

NPR / WGBH, ["Fixing Bias In Algorithms Is Possible, And This Scientist Is Doing It"](https://www.wgbh.org/news/science-and-technology/2018/12/09/fixing-bias-in-algorithms-is-possible-and-this-scientist-is-doing-it), Heather Goldstone and Elsa Partan, Dec 9, 2018.

## Hobbies

In my free time, I enjoy [long distance running](https://twitter.com/irenetrampoline/status/986059482022273024), [reading books](./reading), and [discussing AI ethics](https://mitaiethics.github.io).