---
title: ""
date: 2019-07-28T23:32:34-04:00
---

<img class="profile-picture" src="irene.jpg">

Irene is a Ph.D. candidate at MIT CSAIL, advised by Professor David Sontag in the Clinical Machine Learning group. Her research focuses on robust machine learning for real-world problems, particularly when focused on healthcare and fairness.

You can email Irene at iychen [at] mit [dot] edu or reach me on [Twitter](http://www.twitter.com/irenetrampoline).

<br>
## Papers

<script>
function absCHF() {
    var x = document.getElementById("abs-fairness");
    if (x.style.display === "none") {
        x.style.display = "block";
    } else {
        x.style.display = "none";
    }
}
</script>

**The Disparate Impacts of Medical and Mental Health with AI.** 
<br>
Irene Y. Chen, Peter Szolovits, Marzyeh Ghassemi. 
<br>
*AMA Journal of Ethics*, February 2019.
<br>
[[pdf](https://journalofethics.ama-assn.org/article/can-ai-help-reduce-disparities-general-medical-and-mental-health-care/2019-02)]

**Why Is My Classifier Discriminatory?** 
<br>
Irene Y. Chen, Fredrik D. Johansson, David Sontag. 
<br>
*NeurIPS 2018*, <b><font color="#B03A2E">Spotlight Presentation (top 4% of submitted papers)</font></b>.
<br>
[<a id="abs-fairness-button" onclick="absCHF()">abstract</a>, [pdf](https://arxiv.org/abs/1805.12002), [slides](/assets/neurips18_slides.pdf), [poster](/assets/neurips18_poster.pdf)]

<div id="abs-fairness" style="display:none;">
<blockquote>Recent attempts to achieve fairness in predictive models focus on the balance between fairness and accuracy. In sensitive applications such as healthcare or criminal justice, this trade-off is often undesirable as any increase in prediction error could have devastating consequences. In this work, we argue that the fairness of predictions should be evaluated in context of the data, and that unfairness induced by inadequate samples sizes or unmeasured predictive variables should be addressed through data collection, rather than by constraining the model. We decompose cost-based metrics of discrimination into bias, variance, and noise, and propose actions aimed at estimating and reducing each term. Finally, we perform case-studies on prediction of income, mortality, and review ratings, confirming the value of this analysis. We find that data collection is often a means to reduce discrimination without sacrificing accuracy.</blockquote>
</div>

**Sources of Unfairness in Intensive Care Unit Mortality Scores.** <br>Irene Y. Chen, Fredrik D. Johansson, David Sontag. <br> *Women in Machine Learning Workshop at NeurIPS 2017.*

## Teaching

At MIT, I served as a Teaching Assistant in Spring 2019 for [Machine Learning for Healthcare](http://mlhc19mit.github.io). Our class was covered by [MIT News](http://news.mit.edu/2019/want-know-what-software-driven-health-care-looks-mit-class-offers-some-clues-0724).

At Harvard, I was awarded the [Derek Bok Center Certificate of Distinction in Teaching](https://bokcenter.harvard.edu/awards) for outstanding teaching evaluations. I served on the teaching staff of the following Harvard classes:

 * Algorithms and Data Structures, Jelani Nelson
 * Microeconomic Theory, Ed Glaeser
 * Multivariable Calculus, Evelyn Hu and Avi Shapiro
 * Differential Equations, Margo Levine and Avi Shapiro
 * Linear Algebra and Real Analysis I, Paul Bamberg
 * Linear Algebra and Real Analysis II, Paul Bamberg


## Hobbies and interests

In my free time, I enjoy [long distance running](https://twitter.com/irenetrampoline/status/986059482022273024), [reading books](http://irenechen.net/reading-list), and [discussing AI ethics](https://mitaiethics.github.io).